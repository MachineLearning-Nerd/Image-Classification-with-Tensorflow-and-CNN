{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run on GPU\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images\\\\batches.meta', 'images\\\\data_batch_1', 'images\\\\data_batch_2', 'images\\\\data_batch_3', 'images\\\\data_batch_4', 'images\\\\data_batch_5']\n",
      "images\\batches.meta\n",
      "{b'num_cases_per_batch': 10000, b'label_names': [b'airplane', b'automobile', b'bird', b'cat', b'deer', b'dog', b'frog', b'horse', b'ship', b'truck'], b'num_vis': 3072}\n",
      "images\\data_batch_1\n",
      "images\\data_batch_2\n",
      "images\\data_batch_3\n",
      "images\\data_batch_4\n",
      "images\\data_batch_5\n"
     ]
    }
   ],
   "source": [
    "# Fetch all the files from the image folder\n",
    "files = glob.glob('images/**')\n",
    "print(files)\n",
    "dictval={}\n",
    "i = 0\n",
    "\n",
    "# Iterate over every file and try to save data to the dictval\n",
    "for file in files:\n",
    "    print(file)\n",
    "    if \"batches.meta\" in file:\n",
    "        # batches.meta contains the data for the label names \n",
    "        # and size of the batch\n",
    "        with open(file,'rb') as fo:\n",
    "            data = pickle.load(fo, encoding='bytes')\n",
    "            print(data)\n",
    "    else:\n",
    "        with open(file, 'rb') as fo:\n",
    "            temp = pickle.load(fo, encoding='bytes')\n",
    "            #print(temp)\n",
    "            if i == 0:\n",
    "                dictval['data']= list(temp[b'data'])\n",
    "                dictval['labels']= list(temp[b'labels'])\n",
    "            else:\n",
    "                dictval['data'] = dictval['data'] + list(temp[b'data'])\n",
    "                dictval['labels'] = dictval['labels'] + list(temp[b'labels'])\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'airplane', b'automobile', b'bird', b'cat', b'deer', b'dog', b'frog', b'horse', b'ship', b'truck']\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# Convert the bytes to the normal string\n",
    "print(data[b'label_names'])\n",
    "labels = [x.decode('utf-8') for x in data[b'label_names']] \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "This is the dimension of the data\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "alldata = dictval['data']\n",
    "alldatalabels = dictval['labels']\n",
    "trainingdata = []\n",
    "def create_training_data():\n",
    "    def reshapedata(imdata, imlabel):\n",
    "        print(len(imdata))\n",
    "#         fig=plt.figure(figsize=(8, 8))\n",
    "        for i  in range(len(imdata)):\n",
    "#         for i  in range(1,5):\n",
    "            # This data is the in the format of 3072 array elements\n",
    "            temp = imdata[i]\n",
    "            #print(temp)\n",
    "            #print(len(temp))\n",
    "            \n",
    "            # To reshape the data\n",
    "            img = np.reshape(temp, (3, 32,32)).T\n",
    "            #print(img.shape)\n",
    "            \n",
    "            # Convert the numpy array into the RGB format\n",
    "            img = Image.fromarray(img, 'RGB')\n",
    "            \n",
    "            # To see the image without correct orientation\n",
    "            #plt.imshow(img)\n",
    "            #plt.show()\n",
    "            \n",
    "            # img is in rotated format, so we need to rotate the image\n",
    "            # to get the original orientation\n",
    "            img = img.rotate(270)\n",
    "            \n",
    "            # Here gray conversion is done: in our application color images are not need because we \n",
    "            # can get the same information in the gray image. \n",
    "            # Benefit of using gray image : It will reduce the calculations by 3(RGB have 3 channels)\n",
    "            img  = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "            img  = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Just to make sure that every image is 32*32\n",
    "#             img = cv2.resize(img, (32,32))\n",
    "            \n",
    "            # To see the image in the correct orientation\n",
    "#             fig.add_subplot(5, 5, i)\n",
    "#             plt.imshow(img)\n",
    "\n",
    "            \n",
    "            \n",
    "            # Just to verify that every label is int\n",
    "            if type(imlabel[i]) != type(2):\n",
    "                continue\n",
    "            class_num = imlabel[i]\n",
    "#             plt.title(labels[class_num])\n",
    "                      \n",
    "#             print(labels[class_num])\n",
    "#             break\n",
    "            # To create training data: \n",
    "            # I have appened the image data and the label\n",
    "            # temp[0]: This is image\n",
    "            # temp[1]: This is label\n",
    "            temp  = [img, class_num]\n",
    "            trainingdata.append(temp)\n",
    "            #break\n",
    "    reshapedata(alldata, alldatalabels)\n",
    "create_training_data()\n",
    "print(\"This is the dimension of the data\")\n",
    "print(len(trainingdata))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to make the data shuffled randomly\n",
    "import random\n",
    "random.shuffle(trainingdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 9\n",
      "label = 9\n",
      "label = 8\n",
      "label = 0\n",
      "label = 8\n",
      "label = 3\n",
      "label = 0\n",
      "label = 0\n",
      "label = 1\n",
      "label = 7\n",
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# This is just to check whether every data is \n",
    "# append correctly or not.\n",
    "for sample in trainingdata[:10]:\n",
    "    print(\"label = %d\" %sample[1])\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "# This is to store all the images in X\n",
    "# and all the labels in Y\n",
    "for features, label in trainingdata:\n",
    "    X.append(features)\n",
    "    Y.append(label)\n",
    "# To reshape informat of tensorflow\n",
    "X = np.array(X).reshape(-1,32, 32, 3)\n",
    "print(X.shape)\n",
    "# plt.imshow(X[4,:,:,:])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#X = pickle.load(open(\"X.pickle\", 'rb'))\n",
    "#Y = pickle.load(open(\"Y.pickle\", 'rb'))\n",
    "\n",
    "# to normalize the data. \n",
    "X = X.astype('float32')\n",
    "X /= 255.0\n",
    "plt.imshow(X[4,:,:,:])\n",
    "plt.show()\n",
    "# print(X[8,:,:,:])\n",
    "\n",
    "# 60% Training data\n",
    "x_train = X[:30000]\n",
    "y_train = Y[:30000]\n",
    "\n",
    "# 20% Testing data\n",
    "x_test = X[30000:40000]\n",
    "y_test = Y[30000:40000]\n",
    "\n",
    "# 20% Validation data\n",
    "x_val = X[40000:50000]\n",
    "y_val = Y[40000:50000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # model.add(Conv2D(32,(3,3),padding='same', activation='relu'))\n",
    "# # model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
    "# # model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
    "# # model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# # model.add(Dropout(0.2))\n",
    "# model.add(Dense(128,activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "30000/30000 [==============================] - 29s 974us/sample - loss: 2.0916 - acc: 0.2151 - val_loss: 1.8485 - val_acc: 0.3137\n",
      "Epoch 2/100\n",
      "30000/30000 [==============================] - 28s 931us/sample - loss: 1.7096 - acc: 0.3618 - val_loss: 1.5696 - val_acc: 0.4294\n",
      "Epoch 3/100\n",
      "30000/30000 [==============================] - 28s 933us/sample - loss: 1.4892 - acc: 0.4461 - val_loss: 1.4033 - val_acc: 0.4896\n",
      "Epoch 4/100\n",
      "30000/30000 [==============================] - 28s 933us/sample - loss: 1.3561 - acc: 0.5029 - val_loss: 1.3014 - val_acc: 0.5272\n",
      "Epoch 5/100\n",
      "30000/30000 [==============================] - 28s 932us/sample - loss: 1.2672 - acc: 0.5389 - val_loss: 1.2380 - val_acc: 0.5582\n",
      "Epoch 6/100\n",
      "30000/30000 [==============================] - 28s 932us/sample - loss: 1.1918 - acc: 0.5689 - val_loss: 1.1679 - val_acc: 0.5812\n",
      "Epoch 7/100\n",
      "30000/30000 [==============================] - 28s 934us/sample - loss: 1.1318 - acc: 0.5895 - val_loss: 1.1598 - val_acc: 0.5913\n",
      "Epoch 8/100\n",
      "30000/30000 [==============================] - 28s 930us/sample - loss: 1.0691 - acc: 0.6134 - val_loss: 1.0752 - val_acc: 0.6258\n",
      "Epoch 9/100\n",
      "30000/30000 [==============================] - 28s 933us/sample - loss: 1.0193 - acc: 0.6367 - val_loss: 1.0982 - val_acc: 0.6149\n",
      "Epoch 10/100\n",
      "30000/30000 [==============================] - 28s 932us/sample - loss: 0.9759 - acc: 0.6489 - val_loss: 1.1115 - val_acc: 0.6155\n",
      "Epoch 11/100\n",
      "30000/30000 [==============================] - 28s 929us/sample - loss: 0.9315 - acc: 0.6658 - val_loss: 1.0468 - val_acc: 0.6444\n",
      "Epoch 12/100\n",
      "30000/30000 [==============================] - 28s 932us/sample - loss: 0.8786 - acc: 0.6855 - val_loss: 0.9970 - val_acc: 0.6561\n",
      "Epoch 13/100\n",
      "30000/30000 [==============================] - 28s 932us/sample - loss: 0.8507 - acc: 0.6959 - val_loss: 1.0078 - val_acc: 0.6522\n",
      "Epoch 14/100\n",
      "30000/30000 [==============================] - 28s 929us/sample - loss: 0.8147 - acc: 0.7064 - val_loss: 0.9833 - val_acc: 0.6646\n",
      "Epoch 15/100\n",
      "30000/30000 [==============================] - 28s 932us/sample - loss: 0.7857 - acc: 0.7187 - val_loss: 1.0069 - val_acc: 0.6588\n",
      "Epoch 16/100\n",
      "30000/30000 [==============================] - 28s 934us/sample - loss: 0.7465 - acc: 0.7317 - val_loss: 0.9741 - val_acc: 0.6643\n",
      "Epoch 17/100\n",
      "30000/30000 [==============================] - 28s 948us/sample - loss: 0.7274 - acc: 0.7383 - val_loss: 0.9588 - val_acc: 0.6779\n",
      "Epoch 18/100\n",
      "30000/30000 [==============================] - 28s 946us/sample - loss: 0.7092 - acc: 0.7451 - val_loss: 0.9897 - val_acc: 0.6741\n",
      "Epoch 19/100\n",
      "30000/30000 [==============================] - 28s 934us/sample - loss: 0.6676 - acc: 0.7583 - val_loss: 0.9738 - val_acc: 0.6756\n",
      "Epoch 20/100\n",
      "30000/30000 [==============================] - 28s 929us/sample - loss: 0.6567 - acc: 0.7632 - val_loss: 0.9734 - val_acc: 0.6798\n",
      "Epoch 21/100\n",
      "30000/30000 [==============================] - 28s 935us/sample - loss: 0.6138 - acc: 0.7775 - val_loss: 1.0079 - val_acc: 0.6738\n",
      "Epoch 22/100\n",
      "30000/30000 [==============================] - 28s 939us/sample - loss: 0.5960 - acc: 0.7870 - val_loss: 1.0338 - val_acc: 0.6676\n",
      "Epoch 23/100\n",
      "30000/30000 [==============================] - 28s 931us/sample - loss: 0.5691 - acc: 0.7949 - val_loss: 1.0581 - val_acc: 0.6641\n",
      "Epoch 24/100\n",
      "30000/30000 [==============================] - 28s 928us/sample - loss: 0.5517 - acc: 0.7977 - val_loss: 1.0757 - val_acc: 0.6717\n",
      "Epoch 25/100\n",
      "30000/30000 [==============================] - 28s 932us/sample - loss: 0.5259 - acc: 0.8086 - val_loss: 1.0427 - val_acc: 0.6790\n",
      "Epoch 26/100\n",
      "30000/30000 [==============================] - 28s 930us/sample - loss: 0.5024 - acc: 0.8171 - val_loss: 1.0990 - val_acc: 0.6740\n",
      "Epoch 27/100\n",
      "30000/30000 [==============================] - 28s 926us/sample - loss: 0.4875 - acc: 0.8210 - val_loss: 1.0760 - val_acc: 0.6777\n",
      "Epoch 28/100\n",
      "30000/30000 [==============================] - 28s 929us/sample - loss: 0.4685 - acc: 0.8295 - val_loss: 1.1702 - val_acc: 0.6694\n",
      "Epoch 29/100\n",
      "30000/30000 [==============================] - 28s 926us/sample - loss: 0.4475 - acc: 0.8380 - val_loss: 1.1652 - val_acc: 0.6731\n",
      "Epoch 30/100\n",
      "30000/30000 [==============================] - 28s 928us/sample - loss: 0.4387 - acc: 0.8382 - val_loss: 1.2234 - val_acc: 0.6663\n",
      "Epoch 31/100\n",
      "30000/30000 [==============================] - 28s 928us/sample - loss: 0.4211 - acc: 0.8455 - val_loss: 1.1759 - val_acc: 0.6724 -  - ET\n",
      "Epoch 32/100\n",
      "30000/30000 [==============================] - 28s 931us/sample - loss: 0.4212 - acc: 0.8452 - val_loss: 1.1701 - val_acc: 0.6683\n",
      "Epoch 33/100\n",
      "  600/30000 [..............................] - ETA: 25s - loss: 0.3565 - acc: 0.87"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fcdfc8adc4be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Here we are going to train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32md:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    389\u001b[0m       \u001b[0mprev_total_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_width\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynamic_display\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\b'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mprev_total_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32md:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model compilation is done\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "# Here we are going to train the model\n",
    "model.fit(x_train, y_train,batch_size=200, validation_data=(x_val, y_val), epochs = 100)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the test accuracy\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ourmodel.h5')\n",
    "saved_model = tf.keras.models.load_model('ourmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = saved_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predval = 105\n",
    "count = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if np.argmax(y_pred[i]) == y_test[i]:\n",
    "        count +=1\n",
    "accuracy = count/len(y_pred)\n",
    "print(accuracy)\n",
    "# print(np.argmax(y_pred[predval]))\n",
    "# print(y_test[predval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
