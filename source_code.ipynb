{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run on GPU\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images\\\\batches.meta', 'images\\\\data_batch_1', 'images\\\\data_batch_2', 'images\\\\data_batch_3', 'images\\\\data_batch_4', 'images\\\\data_batch_5']\n",
      "images\\batches.meta\n",
      "{b'num_cases_per_batch': 10000, b'label_names': [b'airplane', b'automobile', b'bird', b'cat', b'deer', b'dog', b'frog', b'horse', b'ship', b'truck'], b'num_vis': 3072}\n",
      "images\\data_batch_1\n",
      "images\\data_batch_2\n",
      "images\\data_batch_3\n",
      "images\\data_batch_4\n",
      "images\\data_batch_5\n"
     ]
    }
   ],
   "source": [
    "# Fetch all the files from the image folder\n",
    "files = glob.glob('images/**')\n",
    "print(files)\n",
    "dictval={}\n",
    "i = 0\n",
    "\n",
    "# Iterate over every file and try to save data to the dictval\n",
    "for file in files:\n",
    "    print(file)\n",
    "    if \"batches.meta\" in file:\n",
    "        # batches.meta contains the data for the label names \n",
    "        # and size of the batch\n",
    "        with open(file,'rb') as fo:\n",
    "            data = pickle.load(fo, encoding='bytes')\n",
    "            print(data)\n",
    "    else:\n",
    "        with open(file, 'rb') as fo:\n",
    "            temp = pickle.load(fo, encoding='bytes')\n",
    "            #print(temp)\n",
    "            if i == 0:\n",
    "                dictval['data']= list(temp[b'data'])\n",
    "                dictval['labels']= list(temp[b'labels'])\n",
    "            else:\n",
    "                dictval['data'] = dictval['data'] + list(temp[b'data'])\n",
    "                dictval['labels'] = dictval['labels'] + list(temp[b'labels'])\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'airplane', b'automobile', b'bird', b'cat', b'deer', b'dog', b'frog', b'horse', b'ship', b'truck']\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# Convert the bytes to the normal string\n",
    "print(data[b'label_names'])\n",
    "labels = [x.decode('utf-8') for x in data[b'label_names']] \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "alldata = dictval['data']\n",
    "alldatalabels = dictval['labels']\n",
    "trainingdata = []\n",
    "def create_training_data():\n",
    "    def reshapedata(imdata, imlabel):\n",
    "        print(len(imdata))\n",
    "        for i  in range(0,len(imdata)):\n",
    "        #for i  in range(1,5):\n",
    "            # This data is the in the format of 3072 array elements\n",
    "            temp = imdata[i]\n",
    "            #print(temp)\n",
    "            #print(len(temp))\n",
    "            \n",
    "            # To reshape the data\n",
    "            img = np.reshape(temp, (3, 32,32)).T\n",
    "            #print(img.shape)\n",
    "            \n",
    "            # Convert the numpy array into the RGB format\n",
    "            img = Image.fromarray(img, 'RGB')\n",
    "            \n",
    "            # To see the image without correct orientation\n",
    "            #plt.imshow(img)\n",
    "            #plt.show()\n",
    "            \n",
    "            # img is in rotated format, so we need to rotate the image\n",
    "            # to get the original orientation\n",
    "            img = img.rotate(270)\n",
    "            \n",
    "            # Here gray conversion is done: in our application color images are not need because we \n",
    "            # can get the same information in the gray image. \n",
    "            # Benefit of using gray image : It will reduce the calculations by 3(RGB have 3 channels)\n",
    "            img  = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n",
    "            \n",
    "            # Just to make sure that every image is 32*32\n",
    "            img = cv2.resize(img, (32,32))\n",
    "            \n",
    "            # To see the image in the correct orientation\n",
    "            #plt.imshow(img)\n",
    "            #plt.show()\n",
    "            \n",
    "            # Just to verify that every label is int\n",
    "            if type(imlabel[i]) != type(2):\n",
    "                continue\n",
    "                \n",
    "            # Stored the labels in another variable\n",
    "            class_num = imlabel[i]\n",
    "            \n",
    "            #print(labels[class_num])\n",
    "            \n",
    "            # To create training data: \n",
    "            # I have appened the image data and the label\n",
    "            # temp[0]: This is image\n",
    "            # temp[1]: This is label\n",
    "            temp  = [img, class_num]\n",
    "            trainingdata.append(temp)\n",
    "            #break\n",
    "    reshapedata(alldata, alldatalabels)\n",
    "create_training_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to make the data shuffled randomly\n",
    "import random\n",
    "random.shuffle(trainingdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 9\n",
      "label = 0\n",
      "label = 0\n",
      "label = 2\n",
      "label = 2\n",
      "label = 1\n",
      "label = 1\n",
      "label = 3\n",
      "label = 3\n",
      "label = 6\n",
      "(50000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# This is just to check whether every data is \n",
    "# append correctly or not.\n",
    "for sample in trainingdata[:10]:\n",
    "    print(\"label = %d\" %sample[1])\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "# This is to store all the images in X\n",
    "# and all the labels in Y\n",
    "for features, label in trainingdata:\n",
    "    X.append(features)\n",
    "    Y.append(label)\n",
    "# To reshape informat of tensorflow\n",
    "X = np.array(X).reshape(-1,32, 32, 1)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pickle.load(open(\"X.pickle\", 'rb'))\n",
    "#Y = pickle.load(open(\"Y.pickle\", 'rb'))\n",
    "\n",
    "# to normalize the data. \n",
    "X = X/255.0\n",
    "\n",
    "# 60% Training data\n",
    "x_train = X[:30000]\n",
    "y_train = Y[:30000]\n",
    "\n",
    "# 20% Testing data\n",
    "x_test = X[30000:40000]\n",
    "y_test = Y[30000:40000]\n",
    "\n",
    "# 20% Validation data\n",
    "x_val = X[40000:50000]\n",
    "y_val = Y[40000:50000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization \n",
    "model = Sequential()\n",
    "model.add(Conv2D(256, (3,3), input_shape = X.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), input_shape = X.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), input_shape = X.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), input_shape = X.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(30))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation(\"relu\"))\n",
    "# Output layers\n",
    "model.add(Dense(10))\n",
    "# Here we will get the outputs in probability \n",
    "model.add(Activation(\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "30000/30000 [==============================] - 31s 1ms/sample - loss: 0.0234 - acc: 0.9926 - val_loss: 5.3582 - val_acc: 0.5029\n",
      "Epoch 2/100\n",
      "30000/30000 [==============================] - 30s 992us/sample - loss: 0.0078 - acc: 0.9978 - val_loss: 5.7891 - val_acc: 0.5187\n",
      "Epoch 3/100\n",
      "30000/30000 [==============================] - 29s 982us/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 5.9610 - val_acc: 0.5087\n",
      "Epoch 4/100\n",
      "30000/30000 [==============================] - 28s 947us/sample - loss: 0.0170 - acc: 0.9950 - val_loss: 5.7160 - val_acc: 0.5084\n",
      "Epoch 5/100\n",
      "30000/30000 [==============================] - 29s 961us/sample - loss: 0.0135 - acc: 0.9964 - val_loss: 5.9941 - val_acc: 0.5138\n",
      "Epoch 6/100\n",
      "30000/30000 [==============================] - 29s 958us/sample - loss: 0.0069 - acc: 0.9978 - val_loss: 6.6645 - val_acc: 0.5134\n",
      "Epoch 7/100\n",
      "30000/30000 [==============================] - 30s 1ms/sample - loss: 0.0144 - acc: 0.9956 - val_loss: 6.2368 - val_acc: 0.5016\n",
      "Epoch 8/100\n",
      "30000/30000 [==============================] - 31s 1ms/sample - loss: 0.0108 - acc: 0.9966 - val_loss: 6.2610 - val_acc: 0.5070\n",
      "Epoch 9/100\n",
      "30000/30000 [==============================] - 31s 1ms/sample - loss: 0.0198 - acc: 0.9938 - val_loss: 6.0697 - val_acc: 0.5047\n",
      "Epoch 10/100\n",
      "30000/30000 [==============================] - 30s 1ms/sample - loss: 0.0112 - acc: 0.9966 - val_loss: 6.1862 - val_acc: 0.5121\n",
      "Epoch 11/100\n",
      "30000/30000 [==============================] - 30s 987us/sample - loss: 0.0097 - acc: 0.9970 - val_loss: 6.3508 - val_acc: 0.5072\n",
      "Epoch 12/100\n",
      "30000/30000 [==============================] - 30s 989us/sample - loss: 0.0109 - acc: 0.9967 - val_loss: 6.7627 - val_acc: 0.4874\n",
      "Epoch 13/100\n",
      "30000/30000 [==============================] - 29s 955us/sample - loss: 0.0205 - acc: 0.9937 - val_loss: 6.3470 - val_acc: 0.5210\n",
      "Epoch 14/100\n",
      "30000/30000 [==============================] - 29s 959us/sample - loss: 0.0066 - acc: 0.9979 - val_loss: 6.5087 - val_acc: 0.5168\n",
      "Epoch 15/100\n",
      "30000/30000 [==============================] - 29s 955us/sample - loss: 0.0040 - acc: 0.9989 - val_loss: 6.9803 - val_acc: 0.5149\n",
      "Epoch 16/100\n",
      "30000/30000 [==============================] - 29s 961us/sample - loss: 0.0122 - acc: 0.9966 - val_loss: 7.0296 - val_acc: 0.5133\n",
      "Epoch 17/100\n",
      "30000/30000 [==============================] - 29s 959us/sample - loss: 0.0216 - acc: 0.9945 - val_loss: 6.2410 - val_acc: 0.5038\n",
      "Epoch 18/100\n",
      "30000/30000 [==============================] - 28s 945us/sample - loss: 0.0139 - acc: 0.9956 - val_loss: 6.8744 - val_acc: 0.5076\n",
      "Epoch 19/100\n",
      "30000/30000 [==============================] - 30s 988us/sample - loss: 0.0053 - acc: 0.9983 - val_loss: 7.2885 - val_acc: 0.5042\n",
      "Epoch 20/100\n",
      "30000/30000 [==============================] - 29s 972us/sample - loss: 0.0071 - acc: 0.9979 - val_loss: 7.1792 - val_acc: 0.5051\n",
      "Epoch 21/100\n",
      "30000/30000 [==============================] - 31s 1ms/sample - loss: 0.0142 - acc: 0.9959 - val_loss: 6.6192 - val_acc: 0.5088\n",
      "Epoch 22/100\n",
      "30000/30000 [==============================] - 29s 975us/sample - loss: 0.0118 - acc: 0.9965 - val_loss: 6.7914 - val_acc: 0.5017\n",
      "Epoch 23/100\n",
      "30000/30000 [==============================] - 29s 963us/sample - loss: 0.0078 - acc: 0.9977 - val_loss: 6.9929 - val_acc: 0.5046\n",
      "Epoch 24/100\n",
      "30000/30000 [==============================] - 29s 952us/sample - loss: 0.0109 - acc: 0.9966 - val_loss: 7.3195 - val_acc: 0.5027\n",
      "Epoch 25/100\n",
      "30000/30000 [==============================] - 29s 956us/sample - loss: 0.0160 - acc: 0.9961 - val_loss: 7.1503 - val_acc: 0.5043\n",
      "Epoch 26/100\n",
      "30000/30000 [==============================] - 28s 947us/sample - loss: 0.0192 - acc: 0.9942 - val_loss: 7.1554 - val_acc: 0.5051\n",
      "Epoch 27/100\n",
      "30000/30000 [==============================] - 29s 966us/sample - loss: 0.0085 - acc: 0.9976 - val_loss: 7.0199 - val_acc: 0.5155\n",
      "Epoch 28/100\n",
      "30000/30000 [==============================] - 30s 1ms/sample - loss: 0.0035 - acc: 0.9988 - val_loss: 7.3924 - val_acc: 0.5161\n",
      "Epoch 29/100\n",
      "30000/30000 [==============================] - 29s 963us/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 7.3095 - val_acc: 0.5164\n",
      "Epoch 30/100\n",
      "30000/30000 [==============================] - 28s 942us/sample - loss: 0.0151 - acc: 0.9959 - val_loss: 7.1981 - val_acc: 0.5145\n",
      "Epoch 31/100\n",
      "30000/30000 [==============================] - 28s 939us/sample - loss: 0.0185 - acc: 0.9947 - val_loss: 6.9039 - val_acc: 0.5117\n",
      "Epoch 32/100\n",
      "30000/30000 [==============================] - 28s 941us/sample - loss: 0.0063 - acc: 0.9982 - val_loss: 7.5815 - val_acc: 0.5063\n",
      "Epoch 33/100\n",
      "30000/30000 [==============================] - 28s 946us/sample - loss: 0.0030 - acc: 0.9990 - val_loss: 7.7270 - val_acc: 0.5113\n",
      "Epoch 34/100\n",
      "30000/30000 [==============================] - 28s 939us/sample - loss: 0.0133 - acc: 0.9955 - val_loss: 7.1317 - val_acc: 0.5113\n",
      "Epoch 35/100\n",
      "30000/30000 [==============================] - 28s 937us/sample - loss: 0.0099 - acc: 0.9967 - val_loss: 7.0840 - val_acc: 0.5120\n",
      "Epoch 36/100\n",
      "30000/30000 [==============================] - 29s 963us/sample - loss: 0.0087 - acc: 0.9974 - val_loss: 7.2029 - val_acc: 0.5128\n",
      "Epoch 37/100\n",
      "30000/30000 [==============================] - 29s 982us/sample - loss: 0.0163 - acc: 0.9952 - val_loss: 7.4222 - val_acc: 0.5124\n",
      "Epoch 38/100\n",
      "30000/30000 [==============================] - 30s 988us/sample - loss: 0.0187 - acc: 0.9942 - val_loss: 6.9566 - val_acc: 0.5070\n",
      "Epoch 39/100\n",
      "30000/30000 [==============================] - 29s 963us/sample - loss: 0.0091 - acc: 0.9977 - val_loss: 7.3264 - val_acc: 0.5103\n",
      "Epoch 40/100\n",
      "30000/30000 [==============================] - 29s 967us/sample - loss: 0.0082 - acc: 0.9974 - val_loss: 7.0867 - val_acc: 0.5048\n",
      "Epoch 41/100\n",
      "30000/30000 [==============================] - 29s 954us/sample - loss: 0.0155 - acc: 0.9962 - val_loss: 7.2470 - val_acc: 0.5010\n",
      "Epoch 42/100\n",
      "30000/30000 [==============================] - 28s 945us/sample - loss: 0.0076 - acc: 0.9978 - val_loss: 7.5591 - val_acc: 0.5105\n",
      "Epoch 43/100\n",
      "30000/30000 [==============================] - 28s 945us/sample - loss: 0.0056 - acc: 0.9983 - val_loss: 7.7349 - val_acc: 0.5100\n",
      "Epoch 44/100\n",
      "30000/30000 [==============================] - 28s 946us/sample - loss: 0.0120 - acc: 0.9964 - val_loss: 7.4664 - val_acc: 0.5120\n",
      "Epoch 45/100\n",
      "30000/30000 [==============================] - 28s 945us/sample - loss: 0.0140 - acc: 0.9960 - val_loss: 7.7195 - val_acc: 0.5078\n",
      "Epoch 46/100\n",
      "30000/30000 [==============================] - 28s 946us/sample - loss: 0.0120 - acc: 0.9967 - val_loss: 7.3831 - val_acc: 0.5012\n",
      "Epoch 47/100\n",
      "30000/30000 [==============================] - 28s 942us/sample - loss: 0.0077 - acc: 0.9975 - val_loss: 7.1299 - val_acc: 0.5139\n",
      "Epoch 48/100\n",
      "30000/30000 [==============================] - 28s 946us/sample - loss: 0.0097 - acc: 0.9971 - val_loss: 7.4991 - val_acc: 0.5079\n",
      "Epoch 49/100\n",
      "30000/30000 [==============================] - 29s 971us/sample - loss: 0.0121 - acc: 0.9964 - val_loss: 7.5466 - val_acc: 0.5072\n",
      "Epoch 50/100\n",
      "30000/30000 [==============================] - 29s 976us/sample - loss: 0.0118 - acc: 0.9963 - val_loss: 7.4281 - val_acc: 0.5121\n",
      "Epoch 51/100\n",
      "30000/30000 [==============================] - 29s 980us/sample - loss: 0.0075 - acc: 0.9979 - val_loss: 7.7114 - val_acc: 0.5071\n",
      "Epoch 52/100\n",
      "30000/30000 [==============================] - 29s 982us/sample - loss: 0.0050 - acc: 0.9988 - val_loss: 7.8271 - val_acc: 0.5037\n",
      "Epoch 53/100\n",
      "30000/30000 [==============================] - 29s 979us/sample - loss: 0.0127 - acc: 0.9964 - val_loss: 7.6067 - val_acc: 0.5091\n",
      "Epoch 54/100\n",
      "30000/30000 [==============================] - 29s 979us/sample - loss: 0.0179 - acc: 0.9950 - val_loss: 7.5516 - val_acc: 0.5070\n",
      "Epoch 55/100\n",
      "30000/30000 [==============================] - 29s 982us/sample - loss: 0.0120 - acc: 0.9969 - val_loss: 7.2900 - val_acc: 0.5099\n",
      "Epoch 56/100\n",
      "30000/30000 [==============================] - 29s 979us/sample - loss: 0.0065 - acc: 0.9980 - val_loss: 7.3546 - val_acc: 0.5060\n",
      "Epoch 57/100\n",
      "30000/30000 [==============================] - 30s 987us/sample - loss: 0.0110 - acc: 0.9970 - val_loss: 7.5646 - val_acc: 0.5142\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 29s 979us/sample - loss: 0.0139 - acc: 0.9957 - val_loss: 7.3174 - val_acc: 0.5090\n",
      "Epoch 59/100\n",
      "30000/30000 [==============================] - 30s 984us/sample - loss: 0.0081 - acc: 0.9977 - val_loss: 7.8134 - val_acc: 0.5005\n",
      "Epoch 60/100\n",
      "30000/30000 [==============================] - 28s 945us/sample - loss: 0.0042 - acc: 0.9988 - val_loss: 7.4358 - val_acc: 0.5125\n",
      "Epoch 61/100\n",
      "30000/30000 [==============================] - 29s 965us/sample - loss: 0.0010 - acc: 0.9997 - val_loss: 8.1139 - val_acc: 0.5092\n",
      "Epoch 62/100\n",
      "30000/30000 [==============================] - 29s 975us/sample - loss: 0.0067 - acc: 0.9978 - val_loss: 7.8431 - val_acc: 0.5047\n",
      "Epoch 63/100\n",
      "30000/30000 [==============================] - 29s 961us/sample - loss: 0.0270 - acc: 0.9929 - val_loss: 6.9947 - val_acc: 0.5132\n",
      "Epoch 64/100\n",
      "30000/30000 [==============================] - 28s 942us/sample - loss: 0.0112 - acc: 0.9969 - val_loss: 7.4885 - val_acc: 0.5054\n",
      "Epoch 65/100\n",
      "30000/30000 [==============================] - 29s 958us/sample - loss: 0.0083 - acc: 0.9975 - val_loss: 7.7250 - val_acc: 0.5107\n",
      "Epoch 66/100\n",
      "30000/30000 [==============================] - 29s 980us/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 7.5867 - val_acc: 0.5180\n",
      "Epoch 67/100\n",
      "30000/30000 [==============================] - 30s 985us/sample - loss: 0.0030 - acc: 0.9990 - val_loss: 7.9789 - val_acc: 0.5090\n",
      "Epoch 68/100\n",
      "30000/30000 [==============================] - 29s 983us/sample - loss: 0.0083 - acc: 0.9977 - val_loss: 8.1427 - val_acc: 0.5070\n",
      "Epoch 69/100\n",
      "30000/30000 [==============================] - 29s 983us/sample - loss: 0.0120 - acc: 0.9965 - val_loss: 7.6002 - val_acc: 0.5032\n",
      "Epoch 70/100\n",
      "30000/30000 [==============================] - 29s 978us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 7.6074 - val_acc: 0.5049\n",
      "Epoch 71/100\n",
      "30000/30000 [==============================] - 29s 977us/sample - loss: 0.0139 - acc: 0.9957 - val_loss: 8.1798 - val_acc: 0.5036\n",
      "Epoch 72/100\n",
      "30000/30000 [==============================] - 29s 983us/sample - loss: 0.0139 - acc: 0.9957 - val_loss: 7.6197 - val_acc: 0.5122\n",
      "Epoch 73/100\n",
      "30000/30000 [==============================] - 29s 982us/sample - loss: 0.0085 - acc: 0.9977 - val_loss: 7.6193 - val_acc: 0.5174\n",
      "Epoch 74/100\n",
      "30000/30000 [==============================] - 30s 984us/sample - loss: 0.0029 - acc: 0.9992 - val_loss: 7.4474 - val_acc: 0.5105\n",
      "Epoch 75/100\n",
      "30000/30000 [==============================] - 29s 983us/sample - loss: 0.0029 - acc: 0.9989 - val_loss: 8.0103 - val_acc: 0.5062\n",
      "Epoch 76/100\n",
      "30000/30000 [==============================] - 29s 983us/sample - loss: 0.0046 - acc: 0.9988 - val_loss: 8.0834 - val_acc: 0.5006\n",
      "Epoch 77/100\n",
      "30000/30000 [==============================] - 29s 980us/sample - loss: 0.0120 - acc: 0.9968 - val_loss: 7.3396 - val_acc: 0.5039\n",
      "Epoch 78/100\n",
      "30000/30000 [==============================] - 29s 983us/sample - loss: 0.0129 - acc: 0.9964 - val_loss: 8.1844 - val_acc: 0.5067\n",
      "Epoch 79/100\n",
      "30000/30000 [==============================] - 29s 978us/sample - loss: 0.0112 - acc: 0.9965 - val_loss: 7.7218 - val_acc: 0.5032\n",
      "Epoch 80/100\n",
      "30000/30000 [==============================] - 29s 982us/sample - loss: 0.0090 - acc: 0.9971 - val_loss: 8.2909 - val_acc: 0.5085\n",
      "Epoch 81/100\n",
      "30000/30000 [==============================] - 30s 984us/sample - loss: 0.0084 - acc: 0.9976 - val_loss: 7.8052 - val_acc: 0.5010\n",
      "Epoch 82/100\n",
      "30000/30000 [==============================] - 30s 984us/sample - loss: 0.0077 - acc: 0.9977 - val_loss: 8.5329 - val_acc: 0.4996\n",
      "Epoch 83/100\n",
      "30000/30000 [==============================] - 30s 984us/sample - loss: 0.0037 - acc: 0.9987 - val_loss: 8.2438 - val_acc: 0.4998\n",
      "Epoch 84/100\n",
      "30000/30000 [==============================] - 29s 968us/sample - loss: 0.0091 - acc: 0.9980 - val_loss: 7.8640 - val_acc: 0.5041\n",
      "Epoch 85/100\n",
      "30000/30000 [==============================] - 29s 974us/sample - loss: 0.0109 - acc: 0.9969 - val_loss: 8.2533 - val_acc: 0.5058\n",
      "Epoch 86/100\n",
      "30000/30000 [==============================] - 29s 966us/sample - loss: 0.0134 - acc: 0.9966 - val_loss: 7.9926 - val_acc: 0.5044\n",
      "Epoch 87/100\n",
      "30000/30000 [==============================] - 29s 974us/sample - loss: 0.0081 - acc: 0.9976 - val_loss: 8.1315 - val_acc: 0.5020\n",
      "Epoch 88/100\n",
      "30000/30000 [==============================] - 29s 973us/sample - loss: 0.0121 - acc: 0.9968 - val_loss: 7.6483 - val_acc: 0.5082\n",
      "Epoch 89/100\n",
      "30000/30000 [==============================] - 29s 979us/sample - loss: 0.0060 - acc: 0.9984 - val_loss: 8.3586 - val_acc: 0.5010\n",
      "Epoch 90/100\n",
      "30000/30000 [==============================] - 30s 985us/sample - loss: 0.0093 - acc: 0.9974 - val_loss: 7.6404 - val_acc: 0.5139\n",
      "Epoch 91/100\n",
      "30000/30000 [==============================] - 29s 981us/sample - loss: 0.0021 - acc: 0.9994 - val_loss: 8.5967 - val_acc: 0.5028\n",
      "Epoch 92/100\n",
      "30000/30000 [==============================] - 30s 987us/sample - loss: 0.0045 - acc: 0.9990 - val_loss: 8.2227 - val_acc: 0.5062\n",
      "Epoch 93/100\n",
      "30000/30000 [==============================] - 29s 966us/sample - loss: 0.0146 - acc: 0.9963 - val_loss: 7.9979 - val_acc: 0.5058\n",
      "Epoch 94/100\n",
      "30000/30000 [==============================] - 29s 974us/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 8.1407 - val_acc: 0.5058\n",
      "Epoch 95/100\n",
      "30000/30000 [==============================] - 30s 985us/sample - loss: 0.0188 - acc: 0.9950 - val_loss: 7.7852 - val_acc: 0.5061\n",
      "Epoch 96/100\n",
      "30000/30000 [==============================] - 30s 984us/sample - loss: 0.0102 - acc: 0.9973 - val_loss: 7.8207 - val_acc: 0.5091\n",
      "Epoch 97/100\n",
      "30000/30000 [==============================] - 30s 984us/sample - loss: 0.0041 - acc: 0.9988 - val_loss: 8.2788 - val_acc: 0.5087\n",
      "Epoch 98/100\n",
      "30000/30000 [==============================] - 30s 995us/sample - loss: 0.0049 - acc: 0.9987 - val_loss: 8.5552 - val_acc: 0.5029\n",
      "Epoch 99/100\n",
      "30000/30000 [==============================] - 30s 995us/sample - loss: 0.0039 - acc: 0.9988 - val_loss: 7.9755 - val_acc: 0.4999\n",
      "Epoch 100/100\n",
      "30000/30000 [==============================] - 29s 983us/sample - loss: 0.0175 - acc: 0.9956 - val_loss: 7.6746 - val_acc: 0.5002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24c42cc9048>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model compilation is done\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "# Here we are going to train the model\n",
    "model.fit(x_train, y_train,batch_size=100, validation_data=(x_val, y_val), epochs = 100)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 394us/sample - loss: 7.5196 - acc: 0.5059\n"
     ]
    }
   ],
   "source": [
    "# To find the test accuracy\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From d:\\net work\\documents\\dheeraj v\\assessment 2\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model.save('ourmodel.h5')\n",
    "saved_model = tf.keras.models.load_model('ourmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = saved_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4267510e-05 2.0714776e-06 6.5638151e-06 9.7470377e-08 9.2137447e-03\n",
      " 3.7651032e-06 4.1262282e-11 9.9074990e-01 2.5082119e-08 9.5169353e-06]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6461\n"
     ]
    }
   ],
   "source": [
    "predval = 105\n",
    "count = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if np.argmax(y_pred[i]) == y_test[i]:\n",
    "        count +=1\n",
    "accuracy = count/len(y_pred)\n",
    "print(accuracy)\n",
    "# print(np.argmax(y_pred[predval]))\n",
    "# print(y_test[predval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
